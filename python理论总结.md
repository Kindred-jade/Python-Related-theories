### 1.python内存管理

~~~
底层C函数PyType_GenericAlloc(c)负责为对象分配内存并初始化类型相关元数据，它会调用_PyObject_Malloc进行决策是否进行内存分配：
内存大小小于等于512字节时：
	（小对象会按照16的倍数的字节数对齐）
	去free的pool里面取一个block存储对象,pool内存池是python中内存管理的重要的一部分
	arena->pool->block
	arena:在运行时按需通过 mmap 匿名映射申请的(通过mmap匿名申请)的连续的大块内存，大小固定为1MB（64位）。CPython 启动时并不预分配固定数量的 arena（通常从 0 开始），启动过程中（导入模块、创建内置对象等）会按需创建 arena，到达 REPL 提示符时可能已经创建了大约 16 个左右（取决于具体构建和导入情况），是内存池的最高层级，管理状态的头信息约为48字节。CPython 使用 双向链表来管理 usable_arenas，这样做的目的是为了在 Arena 状态改变（如从完全满变为有空闲）时，能以 $O(1)$ 的速度将其从当前链表中移除并插入合适的位置,且链表仅用于管理arena元数据,不关联arena的物理内存。
当一个 arena 中的所有 pool 都完全空闲时，CPython 通常会将其归还给操作系统（通过 munmap 或 VirtualFree），但现代实现会保留少量空闲 arena （通常 1～几个）以优化性能，避免频繁的内存映射操作。
		生命周期：
			1.初始化：进程启动时，初始化链表/常量，无内存申请
			2.申请：小对象分配且无free的pool，mmap申请1MB内存，挂载链表
			3.当需要新 Pool 但当前 Arena 已满时，尝试从 usable_arenas 或 unused_arenas 链表中获取合适的 Arena。
			4.闲置：arena中所有pool都free时，挂载到unused_arena，等待复用
			5.释放：进程退出/强制调试/低内存，munmap/VirtualFree归还内存，释放资源
	 pool:内存池中间层级，管理多个Block。pool 大小等于一个系统内存页的大小，避免跨页，减少TLB，提高内存的访问效率。除头信息剩下用于分配block，按szidx（大小索引）分组，各对应一个pool列表，全局pool列表存储pool头指针
(szidx将 8~512 字节的小对象按 16字节对齐划分为 32 个档位（16×1 ~ 16×32），每个档位对应一个全局 pool 链表，这是 pool 按大小索引分组的核心原因。)
	 	生命周期：
	 		1.初始化:当对象申请内存大小<=512，并且申请对应的Block大小的pool没有free的block(如果			arena没有空闲，则先新建arena),创建新pool
	 		2.分配：程序申请对应大小小对象，且全局pool列表有可用pool
	 		3.回收：所属的Block引用计数归0时，触发Block回收
	 		4.闲置：Pool中所有Block均被归还
	 		5.复用：程序申请对应大小的小对象，全局索引中无对应大小Block的Pool
	 		6.销毁：python进程退出或arena被强制释放
	 block:内存池最小单位,单向链表(仅存后继指针，无前驱)，每个空闲的Block的前8字节存储下一个空闲的	Block的内存地址。pool头部的freeblock指针存储该pool内空闲block链表的头指针。分配时直接取头节点，回收时插回头节点，实现了 $O(1)$ 的分配效率，这是 pymalloc 极快的核心原因。
（空闲 Block 在 Pool 中是按照 后进先出（LIFO） 顺序组织的。当一个对象销毁时，其 Block 被插回 freeblock 指针的头部。这种策略能最大化 CPU 缓存命中率，因为最近被释放的内存块往往还在 L1/L2 缓存中。）
	 	生命周期：
	 		1.初始化:pool从arena拆分完成，初始化pool元数据时自动划分
	 		2.分配：程序申请对应大小的小对象，且pool有空闲block
	 		3.复用：所属对象引用计数归0，python解释器触发对象内存回收
	 		4.销毁：所属pool随着arena被强行释放或者python进程退出
		内存大小大于512字节时：
	向系统进行malloc/free申请和释放资源
Cpython内存管理原文：Python 有一个pymalloc内存分配器，专门针对生命周期较短的小对象（小于或等于 512 字节）进行优化。它使用称为“arena”的内存映射，其大小固定为：32 位平台上为 256 KiB，64 位平台上为 1 MiB。对于大于 512 字节的内存分配，它会回退到PyMem_RawMalloc()选择其他 分配方式。
（整数内置有-5到256之间的整数，当定义对应范围内整数，直接指向对应内存地址的对象，与内存池无关）

引用计数：在Cpython中，所有对象都继承自 PyObject 结构体。其头部宏 PyObject_HEAD 中定义了 Py_ssize_t ob_refcnt，用来记录当前对象被引用的次数，当对象被引用时该计数加1，引用结束-1，没有引用则归0，当一个对象的引用计数归 0 时，会先调用该对象的析构函数释放内部资源；若为大对象（>512B），内存直接调用 free 归还给系统堆；若为小对象 (<=512B)，内存会回到 Python 的内存池（Pool/Arena）中重新排队，以便下次快速分配。引用计数的缺点就是可能会造成循环引用的问题，比如A调用B，B调用A，此时用del删除引用计数也无法解决问题，所以python为了解决这个问题引入了gc，并且引用计数还会有性能开销，每次赋值都要进行INCREF和DECREF,这在高性能运算中是有代价的，实际上，Python 的 GIL（全局解释器锁）存在的根本原因之一，就是为了保证多线程下 ob_refcnt 增减操作的线程安全性。如果没有 GIL，引用计数器的并发修改会导致内存混乱。引用计数最大的优点是"实时性（Real-time）"。内存回收的开销平摊在每一次赋值操作中，且对象一旦归零立即销毁。
(Python 3.12 引入了 Immortal Objects (不朽对象)。例如 None, True, False 以及部分单例字符串。它们的引用计数被设为一个特殊的常量，永远不会改变。这样做是为了解决在多进程（Copy-on-Write）环境下，仅仅因为读取这些常量就导致内存页被修改的性能问题和减少多进程 fork 时的 Copy-on-Write 开销，同时也为无 GIL 模式和真正 immutable 对象共享提供基础。

gc:在Cpython中，为了解决循环引用的问题，引入了gc，在Cpython中明确说明：由于垃圾回收器是对 Python 中已有的引用计数机制的补充，因此如果您确定程序不会创建循环引用，则可以通过gc.disable()禁用垃圾回收器。在python中，首先会根据标记清除，标记存活的对象，它分为两个阶段：第一阶段是标记阶段，GC会把所有的存活的对象打上标记，第二阶段是把那些没有存活的对象进行回收。对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象出发，沿着有向边遍历对象，可达的对象标记为活动对象，不可达的对象就是要被清除的非活动对象。在Cpython官方文档可以看到一个generation的概念，我们通常统称为分代回收，分代回收是一种以空间换时间的操作，在Cpython中会将所有容器对象存储，并检查是否构成循环引用，但由于有一些生命周期很长的容器，所以Cpython会先将所有容器对象存储到gen0，gen0的count达到七百时检查一次（默认 700，但可通过环境变量或 gc.set_threshold() 调整），之后存活下来的变量放到gen1,gen0扫描大于等于10次后扫描一次gen1,gen1扫描大于等于10次会扫描一次gen2,他们之中有一个count的概念，这个在各个gen是不一样的，gen0的count代表的是分配次数减去释放次数的对象净增量，gen1和gen2代表的是上一级扫描次数。通常情况下，垃圾回收器发现循环引用后会直接释放内存。但如果这些处于循环引用中的对象定义了特殊的销毁逻辑，GC 就会陷入两难：不知道该先执行哪一个对象的销毁动作。 为了安全起见，Python 会把这些对象扔进 gc.garbage，让你手动处理。（GC 分代阈值是一个三元组 (threshold0, threshold1, threshold2) = (700, 10, 10)，可通过 gc.get_threshold() 查看。）
注意：自从 PEP 442（Python 3.4）引入后，Python 已经解决了带 __del__ 的对象在循环引用中无法回收的问题。现在的 gc.garbage 几乎总是空的，仅限于那些定义了 C 级别的 tp_del 槽位且未适配现代析构协议的对象，或者开启了 gc.DEBUG_SAVEALL 模式。
~~~

### 2.mutable（可变对象）和immutable（不可变对象）

~~~
python虚拟机并不会专门识别mutable和immutable，也并不会针对于mutable或者immutable做出不同的behavior,而是由对象的behavior决定的。在纯python层面是无法区分mutable和immutable，因为二者提供方法并不一致。
区分mutable和immutable示例：
错误示例：
a = 10
lst = []

def funca():
	lst.append(1)

def funcb():
	a = 20
	
print(a)
print(lst)
(输出：10,[1])
错误原因：两者调用方法并不一致，底层实现也不一致
正确示例:
a=10
lst=[]

def funca():
	lst = [1,2,3]
	
def funcb():
	a = 20
	
print(a)
print(lst)
(输出:10,[])
由此可见，在相同的behavior下，python本身对于variable的值并不会改变
特殊示例：
a=10
lst=[]

def funca():
	lst += [1,2,3]
	
def funcb():
	a += 20
	
print(a)
print(lst)
(输出:10,[1，2，3])
这是因为：在python中，list类型+=时的behavior和int类型并不一致
~~~

### 3.容器对象

~~~
container(容器对象)本质上就是存储其他变量引用的变量，在python中，很多情况下会隐式创建容器变量，比如在嵌套作用域下内部函数调用外部函数的free的变量（也就是闭包）的情况下python会隐式创建一个cell object存储free object，并隐式创建一个tuple用于存储cell object，其中cell object和tuple就是容器对象。
~~~

### 4.迭代器

~~~
iterator本身就是一个iterable，它必须定义一个__iter__()方法（返回迭代器自身），并且iterator是惰性的，调用时需要开发者不断取值。iterator是一个表示数据流的对象，可以__next__()函数不断从这个对象中获取新的数据，只能向前不能后退。for loop本身就是针对于可迭代对象通过迭代器取值等一系列操作的语法糖（能被for loop的对象要么有__iter__()方法，要么是一个sequence并有__getitem__()方法），在for loop开始时，python首先会判断对象是否是可迭代对象，之后通过可迭代对象的__iter__()方法生成迭代器进行迭代，当可迭代对象无元素可返回时，for loop会返回stopiterator异常同时隐式捕获并处理（也就是终止迭代），由于这一系列操作，在python中各类型查询速度set≈dict>list(可变类型)，这是由于在取数据时，set会先获取哈希值，再根据哈希值定位桶、对比键，最后返回值，而字典则需要在获取值后再根据key找value，二者时间复杂度均为O(1)，而list需要线性查找，也就是需要通过for loop逐个查找，时间复杂度为O(n)。生成器本身就是特殊的迭代器（是迭代器的语法糖），当函数中有yield时，python会给函数打上标签代表这是一个生成器函数，调用生成器函数会产出一个生成器对象，当执行时遇到yield，python会先将yield后面的值压入栈顶，再暂停生成器并将栈顶的值取出存到返回值内，最后在函数运行完时自动抛出stopiterator终止迭代，send()方法类似与next()，但不同的是send()函数还会传一个值到生成器，也就是说，调用生成器的send方法后，yield会先返回当前yield后面的值，生成器在上次挂起的地方苏醒，yield后面的值变为send进去的值，相当于做了个evaluation，接着生成器会继续正常运行
~~~

### 5.深浅拷贝

~~~
浅拷贝只会copy表层引用，内部引用不变；深拷贝除了copy表层引用，还会copy内部引用。
比如一个容器对象list，他的值为[[1,2,3]]，浅拷贝并在内部的列表append一个4后，不仅浅拷贝后的列表会变成[[1,2,3,4]]，原列表也会更改；但在深拷贝的时候，copy并append后的列表会变成[[1,2,3,4]]，但原列表不变。
这是因为浅拷贝只会copy外部容器的引用，也就是重新造一个容器，但容器内部的元素还是原来的元素，此时如果容器内部有mutable，由于这个容器和copy后的内部容器内存地址一样，所以对这个容器所进行的修改也会同步到原容器；但如果内部元素是immutable或进行添加元素时，由于浅拷贝是创建了一个新的容器，并不会影响原先的容器。
深拷贝则无论如何都不会影响到原列表，这是因为他不仅复制了外部容器，还复制了容器内部元素。
(由于复制一个绝对无法修改的对象是没有意义的，深拷贝碰到不可变元素通常也会返回原引用)
常见的浅拷贝：
	列表的切片操作[:]，list(原列表)、tuple(原元组)、set(原集合)，序列的拼接/乘法(比如list1 + list2、[1,2]*2),解包语法
~~~

### 6.描述器（扩展）

~~~
   在python层级，一般来说，描述符是指具有描述符协议中某个方法的属性值,这些方法包括 __get__()、__set__()和 __delete__()。(官方的描述：A descriptor is what we call any object that defines __get__(), __set__(), or __delete__())。一个descriptor必须定义到type上，也就是必须定义到class的definition里面，如果放在__init__()，他就是一个普通的属性，他的这三个函数(看定义了哪个)就会被python忽略(定义在descriptor中的情况除外).
   这是由于当 Python 执行 obj.name 时，它内部的 C 代码（__getattribute__ 的底层实现）会进行如下判断(描述符由该方法调用__getattribute__())：他会先去找类的namespace(包括父类)，找到了会发现他是一个descriptor(比如定义了__get__())，他会优先去执行descriptor的__get__()方法，并把结果返回。如果第一步没找到，他就会去obj.__dict__()中找，如果找到了，他就会认为这就是用户存的一个值.
   如果每次从 __dict__ 拿个字符串、数字都要判断一下它是不是描述符，Python 的速度会慢得很多。实例字典的设计初衷就是存状态/数据，而类空间的设计初衷是存行为/逻辑。描述符是逻辑，所以必须待在类空间。
   
 class Name:
    def __get__(self, obj, objtype):
        return "Peter"

class A:
    name = Name()

o = A()
o.name = "Bob"
print(o.__dict__)  # 输出: {'name': 'Bob'}
print(o.name)      # 输出: Bob
Name.__set__ = lambda x, y, z: None
print(o.name)      # 输出：Peter
   这里是因为(摘自官方文档):实例查找会扫描命名空间链，优先考虑数据描述符，其次是实例变量，然后是非数据描述符，再然后是类变量，最后才是类变量（__getattr__()如果提供的话)。也就是在只有__get__()时，cpython按照这个顺序查找(数据描述符——>实例字典——>非数据描述符——>类变量)后，发现没有数据描述符，实例字典也为空(因为没有用__init__()初始化实例对象)，最后查找非数据描述符的结果返回了peter。而在执行o.name="Bob"时，Cpython会先判断这是不是一个数据描述符，如果是，不存入实例字典，而是调用他的__set__()方法；如果不是，根据优先级去存入实例字典(不管他有没有__get__()或者普通类变量)。所以根据优先级查找到了实例字典输出就为Bob，但在执行Name.__set__ = lambda x, y, z: None时，尽管只是随意设了一个值，但他有__set__()方法了，如果有__set__()或者__delete__()方法，他就是数据描述符，此时他的优先级最高，所以查找到的值为__get__()返回的值，也就是"Peter"。
~~~

### 7.classmethod和staticmethod的区别和共同点

~~~
classmethod:
	与staticmethod不同，classmethod在调用function之前，会将类引用添加到参数列表前面。无论调用者是对象还是类，这种格式都相同。当方法需要一个类引用，而不需要依赖于存储在特定instance中的数据时，这种行为非常有用。
	类方法既可以对类本身调用（例如 C.f()），也可以对实例调用（例如  C().f()）。实例本身会被忽略，但其所属的类会被保留。如果对派生类调用类方法，则派生类对象会作为隐式第一个参数传递。
	classmethod装饰的方法在访问时可以看到他的函数类型为<bound method Test.clsmethod of <class 'Test'>>，代表着他是一个绑定方法，身份为Test类下的clsmethod，归属于class 'Test'。
	在单例模式(一种设计模式)的情况下，通常采用classmethod，而不是staticmethod。这是因为如果有继承需求的话，即需要一个子类继承父类，classmethod的cls代表的类会灵活改变，而staticmethod只能写死，也会因为这个造成硬编码的情况。可能很多时候并没有这个需求，但classmethod在实现"可继承的单例工厂"时非常有用，比如：
	import threading

    class BaseGenerator:
        _instance = None
        _lock = threading.Lock()

        def __init__(self):
            self.counter = 0

        @classmethod
        def get_instance(cls):
            if cls._instance is None:
                with cls._lock:
                    if cls._instance is None:
                        cls._instance = cls()
            return cls._instance

        def next_id(self):
            self.counter += 1
            if self.counter == 3:
                raise Exception("时钟回拨异常！")
            return f"ID-{self.counter}"

    class RobustGenerator(BaseGenerator):
        def next_id(self):
            try:
                return super().next_id()
            except Exception as e:
                print(f"【容错机制触发】拦截到异常: {e}。执行降级方案...")
                return "ID-FIXED-RECOVERY"
                
    gen1 = BaseGenerator.get_instance()
    print(f"基础版调用: {gen1.next_id()}")

    gen2 = RobustGenerator.get_instance()

    print(f"增强版第一次: {gen2.next_id()}")
    print(f"增强版第二次: {gen2.next_id()}") # 这一次会触发父类的异常，但被子类捕获
staticmethod:
	用staticmethod装饰的函数很像自由函数，但首先他是可以被子类继承的，其次它属于类的范围，他的类型为<function Test.static...>，可以看到，他是谁也没绑定的，但他的身份是Test下的static。当开发者调用他时，他内部的__get__()方法会告诉python让python不往里面塞cls或者self。他的好处在于：首先，他提供了一个更好的封装，这个函数现在从属于一个类；其次，我们并不用建立一个类的object，可以直接用类调用static method。当我们想把功能绑定在一个类上，而不是对象上的时候，他就很有用。他可以直接通过cls().func()调用。
	他的应用场景，举个例子：当我传来两个参数，我需要对这两个参数做校验再计算时，校验我完全不需要依赖于任何其他，我只要有参数就可以了，这时就可以用staticmethod
	
共同点：首先，他们都是装饰器，按照装饰器的理念实际上都是建立了一个descriptor的实例(按装饰器的等价形式转换为f=staticmethod(f))，其次他们都在类空间
~~~

### 8.await

~~~
如果一个对象可以在表达式中使用，我们称它为可等待await对象。许多asyncio API都支持接受可等待对象
可等待对象总共有三种类型:coroutines，tasks，Futures.

coroutines:在python中，通常有async def定义的函数就是一个coroutine function，他会返回一个coroutine object，python的协程可以在很多点上暂停和恢复(协程是子程序的一种更通用的形式。子程序在某个点进入，在另一个点退出。而协程可以在多个不同的点进入、退出和恢复)。python中协程是可等待的，因此可以被其他协程等待。coroutine的本质就是生成器。

task:任务用于并发调度协程，当协程被封装到Task 中时，例如，asyncio.create_task()协程会被自动调度到稍后运行(asyncio.create_task()用于创建单个任务，多个任务要用asyncio.gather())。task实际上是future的子类

futures：futures用于连接底层基于回调的代码和高层async/await代码。asyncio.Future是一个对象，它代表一个现在还没有完成，但预计在未来会产生结果的异步操作，它不是线程安全的。在初始时他是空的，当异步操作结束时，有人会像Future填入结果。future本身只存结果，没有运行的能力，task继承了future存结果的能力，同时他还能驱动协程运行。通常，Futures 用于使底层基于回调的代码（例如，使用 asyncio 传输实现的协议）能够与高层 async/await 代码互操作。

await底层其实是生成器实现的,在pep492明确标注：协程内部基于生成器，因此它们共享相同的实现。与生成器对象类似，协程也具有throw(), send() 和 close()方法，StopIteration 和 GeneratorExit在协程中扮演着相同的角色，throw(), send() 的方法用于将值推送到类似 Future 的对象中并引发错误。
在底层的result.add_done_callback(self.__wakeup,context=self.__context)，__wakeup 内部本质上是再次调用了 self.__step,result是需要先完成的task或者future。
如果这个生成器结束了，不管它等待的是coroutine、task又或者是future，在结束时它底层的retval(变量)的值就是NULL并把返回值保存到val里 ，之后将val放到了栈顶，当await后面的生成器结束的时候，他就会继续去运行下一个Bytecode。在async def里，当return了一个东西时，等价于raise了一个StopIteration，之后把那个StopIteration里面的value设置成你这个返回值(这里是一个语法糖)。

关于task如何做到并发和相互等待：
	当开发者建立一个task时，底层代码会有一个_loop.call_soon,他会让event loop在下一个iteration的时候run一下__step函数，当task发现开发者await了一个其他的task或者future，他会在那个task或者future标记一下，他会告诉他依赖的task或者future等你完成了告诉我。
	
callback的实现：
	首先，当awaitable完成时，此时他会raise一个StopIteration，之后他会run到函数set_result()，在这个函数主要call了一个_schedule_callbacks，而他最终的意义就是他完成之后，剩下的可以开始的task都schedule上，此时用的也是call_soon。他不是直接调用callback，而是告诉event loop说，这几个callback可以执行了。
	这样做的原因首先是因为这些callback是运行在不同的上下文的，其次如果直接调用callback可能会产生无法保证公平调度的问题，比如一个callback执行过于深入，耽误了很多其他准备好的任务，导致他们执行不了。所以都交给event loop执行可以保证公平调度并防止堆栈溢出。

注意：
	1.单纯的await coroutine其实是串行执行的，而task会立刻返回一个task对象，当A任务进行时如果有B任务会自动进行B任务
	2.官方建议应用层代码（即普通开发者）应该尽量使用 Task，而只有在编写底层库或框架时（需要手动控制结果填入）才直接操作 Future
	3.通常情况下，不应在面向用户的 API 中直接暴露 Future 对象。创建 Future 对象的推荐方法是调用方法 loop.create_future()。这样，其他事件循环实现就可以注入它们自己优化的 Future 对象实现。
	4.在官方文档明确说明：不应直接调用阻塞（CPU密集型）代码。例如，如果一个函数执行持续1秒的CPU密集型计算，则所有并发的asyncio任务和IO操作都会延迟1秒。可以使用执行器在不同的线程（包括不同的解释器）甚至不同的进程中运行任务，以避免阻塞包含事件循环的操作系统线程。
~~~

### 9.异步协程

~~~
（此题部分可见题8.）
异步本质上就是单线程多协程的event loop，在Python官方标准库中关于asyncio事件循环(Event Loop)的底层 API 参考指南中的前言中明确说明：事件循环是每个 asyncio 应用程序的核心。事件循环运行异步任务和回调函数，执行网络 I/O 操作，以及运行子进程。应用程序开发人员通常应该使用高级的 asyncio 函数，例如 asyncio.run()而很少需要引用循环对象或调用其方法。由于协程是用户态的，他的执行位置在用户空间，并且只需要保存和回复栈指针，他的切换耗时仅仅只有10~100纳秒，并且可以支撑数十万甚至百万的最大并发数；而线程是内核态的，他的执行位置在操作系统内核，由于状态切换、寄存器组和调度算法的开销，线程的切换耗时需要1000~10000纳秒（关于开销可仅了解），受到内存和内核压力的限制，它只能支持数千的并发。所以异步协程更适用于执行I/O密集型任务，因为I/O密集型任务更多体现在等待外部数据或者等待传输数据，关键就在于等待，可以利用协程的低切换耗时、低开销以及高并发去更好的实现，但相较于线程协程其实也有一个缺点，由于协程是用户态的，受单线程调度，他无法直接调用CPU执行CPU密集型任务（协程无法利用多核并行，且缺乏“抢占机制），这个时候就容易阻塞，性能反而不如多线程，并且容易导致系统整体失去响应，所以在执行CPU密集型任务时，具备内核抢占能力的多线程（多进程）反而能更好的提供系统健壮性与响应能力。
asyncio也有信号量，但threading 的信号量是阻塞（Blocking）整个线程，而 asyncio 的信号量是挂起（Suspending）当前的协程，并且asyncio的信号量也有死锁风险，虽然没有线程死锁那么严重，但如果你在一个 async with sem: 内部又去 await sem.acquire()，这个协程会永远等自己释放许可，从而陷入“逻辑死锁”。
~~~

### 10.多线程和线程池以及线程加锁

~~~
多线程：由于GIL全局解释器锁的限制，在python level中，同一时间内，只能有一个线程执行bytecode。
多线程主要是threading模块，该模块提供了一种在单进程内并发运行多个线程的方法，它允许创建和管理线程，从而可以并行（这里主要针对于操作系统level，但python level有GIL锁限制）执行任务并共享内存空间。
多线程即使有GIL锁限制，但依旧会有竞争冒险的问题，这是因为GIL保证的是解释器的完整性，而不是开发者业务逻辑的完整性，GIL的释放是基于时间片或指令计数器的。这意味着一个线程在执行一段bytecode时，可能会在任意一行bytecode之间被强行挂起并切走，这时候可以引入python的一个内置方法queue保证线程安全（queue是FIFO的，虽然list也可以模拟队列，但它本质上并不是线程安全的，而且它的pop()方法也不能像队列那样达到O(1)的时间复杂度，因为list内存地址是连续数组，当他pop也就是头部弹出时会造成空洞，后续元素需要补位）。不过线程安全也有一个代价，虽然queue.Queue()的逻辑操作是O(1)的时间复杂度，但他比底层的deque多了一层同步锁，每次 put() 或 get() 时，线程都必须先获取底层的互斥锁。在超高并发下，由于竞争锁的存在，虽然时间复杂度不变，但实际耗时可能会增加。加这个锁的原因就是为了保证线程安全，因为在多线程环境下，它并不能等价于原子操作，所以需要在deque上加一个同步锁保证原子操作实现线程安全。另外threading+queue也能实现produser-consumer Model，但相较于这种方式构建的produser-consumer Model，在分布式架构更提倡使用rabbitmq构建

线程池：ThreadingExecutor是一个Executor子类，它使用线程池来异步执行调用，他是由一个个的threading.Thread组成的，ThreadPoolExecutor 的底层就是一个 queue.SimpleQueue 或 queue.Queue。线程池虽然好用，但在多线程环境下，它的“副作用”往往比功能本身更需要引起开发者的重视，当一个可调用对象Future等待一个另一个可调用对象，就会发生死锁Future，关于这种情况官方推荐的原则是：不要在 ThreadPoolExecutor 内部进行嵌套等待。现代python的ThreadExecutor会在线程空闲超过一定时间（通常是60秒）后销毁线程，直到降至0或保留最小数量，一旦线程被销毁，其关联的线程栈内存才会真正被还给操作系统。如果你有大量积压在 Queue 里的任务，即便线程没在跑，那些任务对象（Future）也会一直占用内存，这就是为什么如果不设限，容易发生 OOM（每一个 submit 都会创建一个 Future 对象，如果你瞬间提交了 100 万个任务，即使线程池只有 10 个线程，这 100 万个 Future 对象、它们关联的参数数据（args/kwargs）以及闭包环境都会常驻在 Queue 中）。

多线程如何加锁：
	1.threading.lock(互斥锁):Primitive Lock是一种 synchronization原语，并不属于任何特定线程。在Python中，它是目前可用的最低级别同步原语，由_thread扩展模块（该模块提供了处理多线程的底层原语——多个进程共享其全局空间）实现。Primitive Lock只有两种状态："已锁定"和"未锁定"，默认处于未锁定状态。它有两个基本方法acquire() 和 release()，acquire()方法会将锁的状态改为已锁定并立即返回。当锁处于已锁定的状态时，方法acquire()会阻塞，直到另一个线程调用release()方法将其改为已锁定的状态，然后调用acquire()方法将其重置为已锁定的状态并返回。release()方法只能在锁处于"已锁定"的状态调用。它会将锁的状态更改为"已锁定"并立即返回。如果尝试释放一个未锁定的的锁，RuntimeError将会引发异常。锁也支持上下文管理协议。当多个线程阻塞acquire()等待状态变为解锁状态时，当release() 调用将状态重置为解锁状态时，只有一个线程会继续执行；哪个等待的线程会继续执行是未定义的，并且可能因实现而异。所有方法均以原子方式执行。
	2.threading.RLock（可重入锁）:reentrant lock是一种同步原语，同一个线程可以多次获取该锁。其内部除了使用Primitive Lock的锁定/解锁状态，还引入了"持有线程"和"递归级别"的概念。在锁定状态下，某个线程持有该锁；在解锁状态下，没有线程持有该锁（线程调用锁的acquire()方法来锁定它，并调用锁的release()方法来解锁它）。reentrant lock支持上下文管理协议，因此建议使用with而不是手动调用，来处理代码块的获取acquire()和release()获取锁。与Lock的不同，RLock的acquire()/release()调用对可以嵌套。只有最后一个调用 （最外层调用对中的最后一个）才会将锁重置为解锁状态，并允许另一个阻塞的线程 继续执行。acquire()/release()必须成对使用:每次获取锁后，必须在获取锁的过程释放锁。如果没有调用相同次数的释放，会导致死锁
	3.threading.Condition（条件变量）:条件变量是比普通锁更高级的同步方式，他总是与某种锁关联，锁可以手动传入，也可以默认创建。当多个条件变量需要共享同一个锁时，手动传入锁非常有用。锁是条件对象的一部分，无需单独跟踪。条件变量遵循上下文管理协议：使用with语句会获取关联锁，其作用于封闭代码块的持续时间。acquire()/release()还会调用关联锁的相应方法。notify()方法会唤醒正在等待条件变量的线程之一（如果有的话），notify_all() 方法也会唤醒所有正在等待条件变量的线程。notify()and`notify_all()方法不会释放锁；这意味着被唤醒的线程不会立即从调用中返回，而只有在调用wait()的线程 最终放弃对锁的所有权时才会返回。典型的使用条件变量的编程风格会使用锁来同步对共享状态的访问；对特定状态变化感兴趣的线程会wait()反复调用，直到看到所需的状态为止，而修改状态的线程则会 在状态发生变化notify()，notify_all()使其可能成为某个等待线程所需的状态时调用锁。
	4.threading.Semaphore（信号量）:这是计算机史上最古老的同步原语之一，它是早期的荷兰计算机科学家Edsger W. Dijkstra发明。信号量管理一个内部计数器。每次 acquire()调用都会递减该计数器，每次release() 调用都会递增该计数器。该计数器永远不会小于零；当信号量acquire() 发现计数器为零时，它会阻塞，等待其他线程调用该计数器 release()。信号量也支持上下文管理协议。如果你调用 release() 的次数超过了 acquire() 的次数，计数器会一直涨，有计数器溢出的风险。
	5.threading.Event（事件）：这是线程间通信最简单的机制之一,一个线程发出事件信号，其他线程等待该事件发生。事件对象管理一个内部标志，该标志可以通过方法设置为 true set()，也可以通过方法重置为 false clear() 。该wait()方法会阻塞，直到标志变为 true。
~~~

### 11.多进程与进程池 以及进程间的通信

~~~
多进程在python库中主要使用multiprocessing，它是一个支持使用类似于threaing模块的API生成进程的软件包，主要提供了本地和远程并发。通过使用multiprocessing可以有效的绕过GIL锁实现并行的效果(GIL只能锁住进程中每个线程的运行，但多个进程无法锁住)。在multiprocessing库中，主要通过Process对象并调用其start()方法生成的。多进程一定需要写if __name__ == '__main__'，这是因为在Windows系统和macOS系统的某些版本下，多进程默认使用spawn模式创建(进程启动模式)，在这种模式下，子进程会重新运行整个主模块的代码来重建环境，如果不加，可能会无限循环运行。并且进程之间是互相隔离的，每个进程都有一个pid，需要借助pipe,共享内存等解决通信问题。

进程总共有三种启动模式:
	1.spawn：父进程会创建一个新的python解释器。子进程只会继承运行进程对象方法所必需的资源。特别是父进程不必要的文件描述符和句柄不会被继承，与其他方式相比，这种方式创建的较慢。官方文档中明确说明这种方式可在 POSIX 和 Windows 平台上运行，并且是Windows 和 macOS 上的默认设置。
	2.fork：父进程会用os.fork()去fork(克隆)python解释器。子进程启动时，实际上与父进程完全相同。父进程的所有资源都会被子进程继承，需要注意fork多线程进程时，他只fork调用fork()那个线程，同时忽略其他所有线程。这种情况建议是，一：永远在开启多线程之前fork。二：用spawn代替他，正如前面说的，spawn每次都会创建一个新的解释器，这种方法虽然慢，但也可以解决这个问题。三：Python 提供了一个高级钩子函数os.register_at_fork()，让你可以在 fork 发生前、后分别执行一些清理逻辑（比如强制重置所有的锁状态），但这种方法难度极高。linux默认启动方式就是fork，因为它的性能优势极大且是 UNIX 的传统。使用fork上下文创建的锁不能传递给使用 spawn或forkserver start 方法启动的进程。
	3.forkserver：程序启动并选择forkserver启动方法时，会创建一个服务器进程。此后，每当需要新进程时，父进程都会连接到服务器并请求其 fork 一个新进程。除非系统库或预加载的导入会产生线程副作用，否则 fork 的服务器进程是单线程的，因此通常可以安全地使用它os.fork()。不会继承任何不必要的资源。
（os.fork():创建一个子进程。将0子进程及其进程 ID 返回给父进程。如果发生错误，OSError则抛出异常。)

进程池：主要通过Pool类的构造函数决定了子进程的初始状态，process决定了进程并行的上限；initializer & initargs主要用于预处理，每个进程启动都会执行一次，常用于连接数据库、加载大模型或者设置全局变量等场景；context决定了子进程是 spawn 还是 fork 出来的。建议通过multiprocessing.get_context('spawn').Pool() 这种方式间接设置，以保证跨平台一致性。需注意进程池无法解决通信问题

Ipc(Inter-Process Communication)：IPC是一个大框架，由于操作系统为了安全，给每个进程划定了独立区域，进程之间相互隔离，进程无法读到另一个进程的变量。因此，必须有一套机制突破这种隔离性实现通信的效果，这种技术统称IPC，其实就是协议族。参考POSIX 标准（Python 多进程在 Unix 上的实现依据）或 操作系统经典教材（如《操作系统导论》OSTEP）：由于进程间存在地址空间隔离（Address Space Isolation），操作系统必须提供一组系统调用（System Calls），允许进程在受控的环境下交换数据。这组技术的总称即为 Inter-Process Communication。
（按操作系统基础IPC共有六种通信方式，分别是：管道、命名管道、消息队列、信号量、共享内存以及套接字）
python官方为了方便开发者的使用将进程通信方式分为以下两大类：
共享状态：其实官方一直在强调避免使用共享状态，不过还是举了两种共享状态，原文为：在进行并发编程时，通常最好尽可能避免使用共享状态。在使用多进程时尤其如此。但是，如果您确实需要使用一些共享数据，那么 multiprocessing它提供了几种方法来实现这一点。
	Shared memory：可以使用 Value 或 Array 将数据存储在共享内存映射中。为了更灵活地使用共享内	存，可以使用 multiprocessing.sharedctypes支持创建从共享内存分配的任意 ctypes 对象的模块。
	Server process：返回的管理器对象Manager()控制服务器进程，该服务器进程保存 Python 对象，并允许其他进程使用代理来操作这些对象。Manager () 返回的管理器将支持列表、字典、集合、Namespace、Lock（锁）、RLock（可重入锁）、Semaphore（信号量）、BoundedSemaphore（有界信号量）、	Condition（条件变量）、Event（事件）、Barrier（屏障）、Queue（队列）、Value（值）和 Array（数	 组）这些类型。服务器进程管理器比使用共享内存对象更灵活，因为它们可以支持任意对象类型。此外，单个管理器可以通过网络被不同计算机上的进程共享。然而，它们的速度比使用共享内存慢。
(而且共享状态的方式很容易造成死锁和竞争冒险的问题，就像多线程)
进程间交换对象：multiprocessing支持进程间两种类型的通信通道
	queue:Queue几乎是queue.Queue的克隆版。队列是线程安全和进程安全的。任何放入multiprocessing队列的对象都会被序列化。Queue结合了管道的传输能力、信号量的同步能力以及序列化的转换	   能力。它在底层利用了Posix的基础模式，但在应用层提供了所有权明确的高级语义，从而避免了直接操作共享内存的死锁和竞争风险
	pipe:主要使用Pipe()函数，它会返回一对通过管道连接的连接对象，默认情况下该管道为双工（双向）连接。Pipe () 返回的两个连接对象代表着管道的两端。每个连接对象都具有 send () 和 recv () 等方法。需	 要注意的是，如果两个进程（或线程）尝试同时从管道的同一端进行读取或向其写入数据，管道中的数据可能会损	坏。当然，进程同时使用管道的不同端时，不存在数据损坏的风险。
（其实除了以上官方文档所说的种类，python还有很多隐藏的或底层的通信模式，比如信号、套接字、文档映射和底层连接，但由于有更简洁的用法或者更好的取代方式所以这里并不提及）

进程信号量：进程信号量是多进程环境下的“通行证管理器”。 它的核心作用是控制多个独立进程对共享资源的访问数量。他的核心功能主要是限流和互斥，信号量会维护一个计数器，开发者可以设置他的初始值，每当一个进程acquire，计数器减一，当计数器为0时，其他想进来的线程必须排队阻塞，如果你把它初始值设为1，他就会退化成进程level的互斥锁，同一时间只能有一个进程能操作资源
~~~

### 12.什么是posix(扩展)

~~~
POSIX (Portable Operating System Interface) 是一套国际标准。
早期的计算机厂家（如 IBM, HP, Sun）都有自己的操作系统，API 全都不一样。代码在这台机器能跑，换一台就得重写。POSIX 规定了一套标准的“函数名”和“行为”。比如，它规定了创建一个新进程的函数必须叫 fork()。比如在操作系统中Unix/Linux/macOS就是POSIX的忠实拥护者，而windows不属于POSIX家族，这也是为什么python在windows不能直接用fork的原因。（macos默认使用spwan的原因是因为macos的系统框架对fork并不友好，甚至认为它不安全，这里不细说）
~~~

### 13.多进程、多线程以及协程的区别以及选择

~~~
区别：多进程和多线程是内核态的，其中进程是有独立的内存空间，更"笨重"，并且相互隔离的特性会导致进程的通信困难，但进程可以绕过Python的Gil限制，从而实现并行；线程则是系统调度的最小单位，更轻量级，但线程受Gil锁限制，在同一时刻只能有一个线程操作bytecode，并且线程需要考虑线程安全问题，也就是竞争冒险的问题，竞争冒险问题本质上就是由于多线程并发执行任务时，因为缺乏协调，无法保证任务执行的原子性和任务执行顺序，同时还会有死锁问题，死锁问题本质上就是因争夺资源而相互等待，举个例子比如A线程拿着钥匙1等钥匙2，线程B拿着钥匙2等钥匙1。协程则是三者中最轻量级的，它是用户态的，受到用户调度而不是操作系统调度，协程更多时候用于构建单线程多协程的异步event loop，协程其实也有死锁和竞争冒险问题，但与线程不同，它虽然本质上是在单线程内运行，避免了物理上的同时操作，不过由于协程在await一个协程后会自动挂起的特质，如果多个协程在同一个变量进行操作就会产生竞争冒险的问题，如果多个协程在同一个event loop循环互等就会死锁。

接下来我会从两个维度剖析关于多进程、多线程以及协程的选择：
I/O密集型：首先是在I/O密集型任务的考虑，在python中由于多进程过于"笨重"，这里首先不考虑，所以可以考虑多线程和异步协程。多线程可以实现并发，并且可以通过queue保证线程安全、用线程池节省切换开销，但多线程是内核态的，由于状态切换、寄存器组和调度算法的开销，线程的切换耗时需要1000~10000纳秒，而协程只需要考虑保存和回复栈指针，耗时仅仅只有10-100纳秒，所以通常如果没有CPU计算操作采用异步协程进行操作是更合适的，但如果有CPU操作本质异步协程还是单线程，协程并没有直接操作CPU的能力，此时就会阻塞，如果任务混杂了这种不可避免的同步计算逻辑，我就会选择多线程，因为多线程是抢占式的，操作系统能帮我‘强行’分片时间，防止单个计算任务把整个系统的响应性给带崩了。同时如果是用request库进行爬取数据时，也需要用多线程，因为request库是同步阻塞的，但相反，用httpx库爬取时就可以用异步，因为他原生支持异步。

CPU密集型：CPU密集型操作时就不考虑协程了，因为协程是用户态，没有直接操作CPU的能力，这里考虑多进程和多线程。如果当前业务场景不考虑内存消耗，可以直接采用多进程，因为多进程可以绕过GIL锁实现并行，并行计算无疑是比并发更快的，但多进程需要考虑通信，可以使用官方更为推荐的Queue实现通信再加上进程池优化，他结合了管道的传输能力、信号量的同步能力以及序列化的转换能力。它在底层利用了Posix的基础模式，但在应用层提供了所有权明确的高级语义，从而避免了直接操作共享内存的死锁和竞争风险；但如果需要考虑内存消耗可以退而求其次之选择多线程并发执行操作，多线程也是内核态的，可以直接操作CPU，需要考虑的就是死锁和竞争冒险问题，在分布式架构，可以直接用rabbitmq构建队列,在 RabbitMQ 的内部，队列的操作是天然线程安全且原子化的，并且还可以使用他的队列构建produser-consumer Model防止死锁问题。另外要说的是，在python中有很多库是C level的，比如numpy、pandas，它们在进入 C 代码段执行前，会调用 Py_BEGIN_ALLOW_THREADS 宏来主动释放 GIL，有个例子就是我在hash加密的时候采用argon2id，他的官方库是C level的，C level本身不需要受到python level的GIL的约束，并且本身加密就有一定内存开销，这时候采用多线程的线程池肯定更优于多进程。
~~~

### 14.装饰器

~~~
装饰器就是一个函数返回另一个函数的函数(其实也可以不返回，但基本不这么用)，装饰器语法本身就是语法糖，以下两个函数定义在语义上是等价的：
	def f(arg):
    	...
	f = staticmethod(f)

	@staticmethod
	def f(arg):
   	 ...
类中也存在类似的概念，是类装饰器（还有带参装饰器等，这里不过多列举）。函数定义可以被一个或多个装饰器包裹，装饰器表达式在函数定义时，在其定义的作用域内进行求值，结果必须是一个callable，该对象会以函数对象作为唯一参数被调用。返回值绑定到函数名，而不是函数对象本身。多个装饰器可以嵌套使用，包装时是自下而上的，调用时执行顺序是自上而下的。虽然装饰器不一定要用闭包，但绝大多数带状态的装饰器都依赖闭包。当高阶函数返回内部函数时，内部函数会携带一个 __closure__ 属性，其中包含了指向外部作用域变量（如被装饰函数 func）的 cell 对象引用。同时他还能延长生命周期，即使外部装饰器函数执行完毕并出栈，由于内部函数（wrapper）仍持有这些变量的引用，这些变量不会被引用计数机制销毁。
~~~

### 15.多继承MRO

~~~
mro全程是method resolution order，中译为方法解析顺序，在Cpython中多继承的顺序总是遵循mro，mro底层则是C3算法（线性的）。在Cpython里一个class如何从他的父类里面找，应该优先使用哪个父类的函数，这个顺序就叫做mro。mro会把每一个类做一个linearization(线性化)，也就是把他继承的所有类和他自己进行排队，并保证自己是最高优先级，当开发者想调用一个方法或查询一个数据时按这个队列优先级顺序从前往后找，获取一个类的mro有两种方式，分别是class.__mro__()和class.mro()。

C3算法：Cpython从2.3版本以来，mro底层的实现方式就一直采用C3算法，C3算法是在96年被提出，最开始是用在Dylan语言上。C3算法之所以叫C3，是因为他在三个属性下都可以做到consistent(一致性)：
1.a consistent extended precedence graph(一个一致的扩展性优先级图)：“一阶一致的扩展优先级图”是指将“子类→父类”的直接继承关系与“父类i→父类i+1（下一个父类）”的局部顺序相结合，构成的无环有向图（DAG），它是C3算法能够进行拓扑排序并生成唯一线性序列的逻辑前提。
2.preservation of local precedence order(保持局部优先级顺序):当一个class继承了多个class时，会优先使用写在前面的class，比如说M同时继承了A和B，由于A写在前面，所以根据此原则优先使用A的方法，除了M要保持这个特性，m的所有sub class也都要保持这个特性，也就是只要有class继承了M，就要保持这个特性。
3.fitting a monotonicity criterion(适配单调性准则)：任何一个class，他使用的方法，必须来自于他的直接父类。如果类A在类B的继承顺序中排在类C之前，那么在B的任何子类中，A也必须排在C之前。
(这个算法类似于找到拓扑次序)
注意：C3算法无论如何计算，最终都指向所有类的顶级父类object。
~~~

### 16.对称加密和非对称加密

~~~
对称加密只有一个密钥，客户端向服务器端发起申请并携带密钥，但考虑到这样安全性无法得到保障，它会造成一个技术痛点，即难以让双方安全地同步同一个密钥而不被中间人截获，所以人们发明了非对称加密，非对称加密是公钥＋私钥的结构，有两种方式，第一种是在取到私钥加密的数据前还需要用公钥进行解密，而私钥解密的方式只有服务器端持有，用于保证数据的保密性，第二种是用私钥加密，公钥进行解密，用于保证数据的真实性并防篡改（公钥是完全公开的）。对称加密的性能相较于非对称加密更快，常用于一些大文件加密等，但他的密钥难以保障安全性，如果无法解决这个问题的话对称加密安全性远不如非对称加密（但如果能保障这种的安全性，比如AES-256甚至相较于RSA这种非对称的安全性在数学角度甚至更高）。常见的对称加密有AES，DES，3DES，非对称加密有RSA，ECC等。

在实际项目中可能会有中间人攻击（英语：Man-in-the-middle attack，缩写：MITM）的问题，中间人攻击的密码学和计算机安全领域是指攻击者与通讯的两端分别建立独立的联系，并交换其所收到的数据，使通讯的两端认为他们正在通过一个私密的连接与对方直接对话，但事实上整个会话都被攻击者完全控制。在中间人攻击中，攻击者可以拦截通讯双方的通话并插入新的内容。一个中间人攻击能成功的前提条件是攻击者能将自己伪装成每一个参与会话的终端，并且不被其他终端识破，所以现在大多数都采用CA证书进行认证，比如SSL。
~~~

### 17.python的作用域和命名空间

~~~
命名空间：命名空间是名称到对象的映射，目前大多数命名空间都以python字典的形式实现，命名空间包括：内置名称集合(包含魔术方法和内置函数)；模块内的全局名称以及函数调用的局部名称，从某种意义上说，对象的属性集合也构成一种命名空间。关于命名空间，需要了解的重要一点是，不同命名空间中的名称之间绝对没有关联；例如，两个不同的模块都可以定义一个名为 maximize 的函数，且不会产生混淆 —— 模块的使用者必须在该函数名前加上模块名作为前缀。在Cpython中所有"."后面都是属性，比如z.real，real就是z对象的属性，在这种情况下，模块的属性和模块中定义的全局名称之间存在直接的映射关系：它们共享同一个命名空间（存储在模块的 __dict__ 中）。命名空间创建的时间点不同，生命周期也不同。包括内置名称在python解释器启动时创建，并且永久不会被删除；模块的全局命名空间在读取模块定义时创建；解释器顶层调用执行的语句，无论是从脚本文件读取的还是交互式输入的，都被视为名为__main__的模块的一部分，因此它们有自己的全局命名空间。（内置名称实际上也存在于一个模块中，这个模块名为 builtins）；函数的命名空间在函数被调用时创建，并在函数返回或抛出未在函数内部处理的异常时才被删除，递归各自拥有局部的独立命名空间。

作用域：作用域是python程序中可以直接访问命名空间的一段文本区域，“直接访问”指的是，对名称的非限定引用会尝试在命名空间中查找该名称（LEGB）。虽然作用域是静态确定的，但它们是动态使用的。在执行过程中，都有3到4个嵌套作用域，它们命名空间可以直接访问（LEGB）：local（局部）->enclosing（嵌套作用域）->global（全局）->builtins（构造函数），如果将一个名称声明为全局变量，则所有引用和赋值都会直接指向包含模块全局名称的global作用域。Python 的一个特殊之处是：如果没有生效的 global（全局）或 nonlocal（非局部）语句，对名称的赋值总是会进入最内层的作用域。赋值不会复制数据 —— 它们只是将名称绑定到对象上。删除操作也是如此：语句 del x 会从局部作用域所引用的命名空间中移除 x 的绑定。实际上，所有引入新名称的操作都会使用局部作用域：特别是 import 语句和函数定义会在局部作用域中绑定模块或函数名称。global 语句可用于指明特定变量存在于全局作用域中，并且应该在那里重新绑定；nonlocal 语句则表明特定变量存在于一个外层作用域中，且应该在那里重新绑定。
~~~

### 18.闭包是什么

~~~
说到闭包就必须要讲Free Variable，如果一个名称在一个函数中被绑定，它就是一个局部变量，如果调用了但没有定义，他就是Free Variable。其实Free Variable从另一个角度来看就是闭包场景最关键的嵌套作用域变量，内部函数调用外层嵌套作用域下的变量就是闭包。判断闭包的方式有很多种，我常常采用对字节码进行分析的方案，也就是通过dis包对python执行的字节码进行反汇编，在闭包场景下，常见的反汇编指令有比如：LOAD_DEREF，用于加载一个Free Variable，会在内部作用域中调用外部嵌套作用域下的变量时出现；STORE_DEREF，当对一个Free Variable进行修改操作，会从栈顶弹出一个对象，存入cell object;LOAD_CLOSURE用于加载一个cell对象的引用，通常出现在外层嵌套作用域的变量，当外层函数准备创建内部函数（闭包）时，它需要先通过 LOAD_CLOSURE 把那些要“传给”内部函数的变量打包成 Cell 对象，以便存入内层函数的 __closure__ 元组中。

cell object(单元格变量)：在官方的定义中，cell object用于实现被多个作用域引用的变量，对于这样的变量，都会创建一个单元格对象来存储其值。每个引用该值的栈帧的局部变量都包含对外部作用域中使用该变量的单元格的引用。当访问该值时，会使用单元格中包含的值，而不是单元格对象本身。
~~~

### 19.with上下文管理

~~~
with是一种上下文管理协议，它使常见的try语句被封装起来，以便于重用except……finally。
执行with包含一个“项目”的语句的过程如下：
1.对上下文表达式（在 中给出的表达式 with_item）进行求值，以获得上下文管理器。
2.上下文管理器__enter__()已加载以供后续使用。
3.上下文管理器__exit__()已加载以供后续使用。
4.__enter__()调用上下文管理器的方法。
5.如果语句中包含目标with，则将返回值__enter__()赋给该目标。
（该with语句保证，如果__enter__() 方法返回时没有错误，则__exit__()始终会调用该方法。因此，如果在向目标列表赋值期间发生错误，其处理方式与在测试套件中发生的错误相同。请参见下面的步骤 7。）
6.该套件已执行。
7.调用上下文管理器的__exit__()方法。如果异常导致测试套件退出，则异常类型、值和回溯信息将作为参数传递给该方法__exit__()。否则，None将提供三个参数。

如果由于异常而退出程序套件，且该 __exit__()方法的返回值为 false，则重新引发该异常。如果返回值为 true，则抑制该异常，并继续执行该with语句之后的语句。如果由于异常以外的任何原因退出程序套件，__exit__()则忽略返回值，并从所采取的退出类型的正常位置继续执行。

object.enter():输入与此对象相关的运行时上下文。该语句会将此方法的返回值绑定到语句子句with中指定的目标
object.__ exit__ ( self , exc_type , exc_value , traceback ) 
退出与此对象相关的运行时上下文。参数描述导致上下文退出的异常。如果上下文在没有异常的情况下退出，则所有三个参数都将为空None。如果抛出了一个异常，并且该方法希望抑制该异常（即阻止其传播），则应返回 true 值。否则，该方法退出时将正常处理该异常。
~~~

### 20.fastapi、flask和django

~~~
fastapi:fastapi首要的就是他的性能高，在性能方面它甚至能与NodeJS和GO媲美，是目前速度最快的Python框架之一，它的底层基于ASGI协议，原生支持异步，同时内置了Pydantic数据验证和文档生成，不过没有内置的ORM，需要额外安装sqlalchemy或toratoise作为它的ORM，并且他还有开箱即用的 Swagger UI 和 ReDoc 交互式文档。fastapi利用了python3.6+的类型提示。在 Flask/Django 中，获取数据库 Session 或当前用户通常需要导入全局对象或在中间件里处理，在 FastAPI 中，你只需在函数参数里声明 user: User = Depends(get_current_user)。框架会自动帮你把需要的组件“注入”进来。这让代码解耦程度极高，测试非常

flask:flask就像是让开发者拼积木，它的核心代码只有几万行，在微服务架构或者小型项目中它可能保证足够高的灵活性，但它只内置了路由和模板引擎，ORM仍需要额外安装sqlalchemy等。它的底层基于WSGI同步协议，性能较为一般。如果用flask做大型项目需要自己设计架构，过于麻烦。

django:django是一个大而全的框架，它基于WSGI协议，同时它的功能及其完善，内置了ORM、Admin管理后台、表单处理和用户认证等。但他相对其他两个框架来讲过于笨重，如果大型项目或者后台管理项目建议采用，但对小型项目来讲用django会显得过于麻烦。Django 3.0+ 开始支持 ASGI，但因为其庞大的第三方插件库（如很多旧的中间件）大多不支持异步，所以并没有完全享受到异步的红利。django框架不同于传统的mvc架构，它有独立的mtv架构,m代表model，t代表template，v代表views
~~~

### 21.什么是GIL

~~~
在CPython中，全局解释器锁（GIL）是一个互斥锁，它确保同一时刻只有一个线程在执行Python的bytecode。此机制通过设置对象模型（包括 dict 等重要内置类型）针对并发访问的隐式安全简化了 CPython 实现。给整个解释器加锁使得解释器多线程运行更方便，其代价则是牺牲了在多处理器上的并行性。不过，某些标准库或第三方库的扩展模块被设计为在执行计算密集型任务如压缩或哈希时释放 GIL，此外，在执行 I/O 操作时也总是会释放 GIL。在 Python 3.13 中，GIL 可以使用 --disable-gil 构建配置选项来禁用，在使用此选项构建 Python 之后，代码必须附带 -X gil=0 或在设置 PYTHON_GIL=0 环境变量后运行。
~~~

